[["what-is-eda.html", "Chapter 2 What is exploratory data analysis? 2.1 What are current interpretations? 2.2 Differentiating from traditional analysis 2.3 The origins of EDA 2.4 Exercises", " Chapter 2 What is exploratory data analysis? Data analysis is a process of cleaning, transforming, inspecting and modelling data with the aim of extracting information. Data analysis includes: exploratory data analysis, confirmatory data analysis, and initial data analysis. Before modelling and predicting, data should first be explored to uncover the patterns and structures that exist. Exploratory data analysis involves both numerical and visual techniques designed to reveal interesting information that may be hidden in the data. However, an analyst must be cautious not to over-interpret apparent patterns, and to make efforts assess the results of a data exploration are reliable for the data being studied and potentially for new data. In a confirmatory data analysis, the focus is on statistical inference and includes processes such as testing hypothesis, model selection, or predictive modelling. Initial data analysis is an important part of this, to check the sometimes strict assumptions made in order to conduct inference. 2.1 What are current interpretations? EDA today is prevalent, and there are many descriptions. It’s interesting to compare and contrast these, in relation to the original scope. From wikipedia In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. In Wickham and Grolemund (2017) EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. The National Institute of Standards and Technology Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to (1) maximize insight into a data set; (2) uncover underlying structure; (3) extract important variables; (4) detect outliers and anomalies; (5) test underlying assumptions; (6) develop parsimonious models; and (7) determine optimal factor settings. The predictive analytics competition site Kaggle says What is Exploratory Data Analysis (EDA)? (1) How to ensure you are ready to use machine learning algorithms in a project? (2) How to choose the most suitable algorithms for your data set? (3) How to define the feature variables that can potentially be used for machine learning? Legacy data analysis software system SAS says EDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint. The massive online John Hopkins EDA class describes it as These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data. General information site Towards Data Science says The purpose of doing the Exploratory Data Analysis or EDA is to find new information in data. The understanding of EDA that practitioners may not aware of, is the EDA uses a visually-examined dataset to understand and summarize the main characteristics of the dataset without having a prior hypothesis or relying upon statistical models. 2.2 Differentiating from traditional analysis 2.2.1 From the case study In 1995 I (Di), as a newly fledged assistant professor, was asked to teach a Business Statistics course. In the process of looking for appropriate examples, I found a book titled “Practical Data Analysis: Case Studies in Business Statistics” by Bryant and Smith (Bryant and Smith 1995). The example on tipping in restaurants caught my eye, because I had always found tipping to be a difficult. Perhaps this was because I was an Australian new to the practice after moving to the USA for graduate school, in 1988 and still uncomfortable with the culture almost a decade later. The types of questions that bounce around mentally, should I tip because the wait staff’s wage comes mostly from this money, how much does the restaurant owner take from the worker, was the service good, have I just been paid and can be generous. What is tipping? According to The basic rules of tipping that everyone should know about: When you’re dining at a full-service restaurant Tip 20 percent of your full bill. When you grab a cup of coffee Round up or add a dollar if you’re a regular or ordered a complicated drink. When you have lunch at a food truck Drop a few dollars into the tip jar, but a little less than you would at a dine-in spot. When you use a gift card Tip on the total value of the meal, not just what you paid out of pocket. About the data The tips case study data is described as follows: In one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990. Food servers’ tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers. and a description of the variables is as reported in Cook and Swayne (2007): Case study procedure The analysis of the data should follow these steps according to the book: Step 1: Develop a model Should the response be tip alone and use the total bill as a predictor? Should you create a new variable tip rate and use this as the response? Step 2: Fit the full model with sex, smoker, day, time and size as predictors Step 3: Refine model: Should some variables should be dropped? Step 4: Check distribution of residuals Step 5: Summarise the model, if X=something, what would be the expected tip Step 1 Calculate tip % as tip/total bill \\(\\times\\) 100 library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() tips &lt;- read_csv(&quot;http://ggobi.org/book/data/tips.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## obs = col_double(), ## totbill = col_double(), ## tip = col_double(), ## sex = col_character(), ## smoker = col_character(), ## day = col_character(), ## time = col_character(), ## size = col_double() ## ) tips &lt;- tips %&gt;% mutate(tip_pct = tip/totbill * 100) #&lt;&lt; Step 2 Fit the full model with all variables tips_lm &lt;- tips %&gt;% select(tip_pct, sex, smoker, day, time, size) %&gt;% lm(tip_pct ~ ., data=.) #&lt;&lt; Summarise the model library(broom) library(kableExtra) ## ## Attaching package: &#39;kableExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## group_rows tidy(tips_lm) %&gt;% #&lt;&lt; kable(digits=2) %&gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 20.66 2.49 8.29 0.00 sexM -0.85 0.83 -1.02 0.31 smokerYes 0.36 0.85 0.43 0.67 daySat -0.18 1.83 -0.10 0.92 daySun 1.67 1.90 0.88 0.38 dayThu -1.82 2.32 -0.78 0.43 timeNight -2.34 2.61 -0.89 0.37 size -0.96 0.42 -2.28 0.02 glance(tips_lm) %&gt;% #&lt;&lt; select(r.squared, statistic, p.value) %&gt;% kable(digits=3) r.squared statistic p.value 0.042 1.479 0.175 🤔 Which variable(s) would be considered important for predicting tip %? Step 3 Refine the model. tips_lm &lt;- tips %&gt;% select(tip_pct, size) %&gt;% #&lt;&lt; lm(tip_pct ~ ., data=.) tidy(tips_lm) %&gt;% #&lt;&lt; kable(digits=2) %&gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 18.44 1.12 16.47 0.00 size -0.92 0.41 -2.25 0.03 \\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\] As the size of the dining party increases by one person the tip decreases by approximately 1%. glance(tips_lm) %&gt;% #&lt;&lt; select(r.squared, statistic, p.value) %&gt;% kable(digits=3) r.squared statistic p.value 0.02 5.042 0.026 \\(R^2 = 0.02\\). This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms. What does \\(R^2 = 0.02\\) mean? \\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model. And \\(R^2 = 0.04\\) is also a very weak model. What do the \\(F\\) statistic and \\(p\\)-value mean? Assume that we have a random sample from a population. Assume that the model for the population is \\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed \\[ \\widehat{tip %} = b_0 + b_1 sexM + ... + b_7 size \\] The \\(F\\) statistic refers to \\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true. What do the \\(t\\) statistics and \\(p\\)-value associated with model coeficients mean? Assume that we have a random sample from a population. Assume that the model for the population is \\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed \\[ \\widehat{tip %} = b_0 + b_1 sexM + ... + b_7 size \\] The \\(t\\) statistics in the coefficient summary refer to \\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true. Model checking Normally, the final model summary would be accompanied diagnostic plots observed vs fitted values to check strength and appropriateness of the fit univariate plot, and normal probability plot, of residuals to check for normality in the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model when the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended. tips_aug &lt;- augment(tips_lm) ggplot(tips_aug, aes(x=.resid)) + #&lt;&lt; geom_histogram() + xlab(&quot;residuals&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(tips_aug, aes(sample=.resid)) + #&lt;&lt; stat_qq() + stat_qq_line() + xlab(&quot;residuals&quot;) + theme(aspect.ratio=1) ggplot(tips_aug, aes(x=.fitted, y=tip_pct)) + #&lt;&lt; geom_point() + geom_smooth(method=&quot;lm&quot;) + xlab(&quot;observed&quot;) + ylab(&quot;fitted&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot(tips_aug, aes(x=size, y=tip_pct)) + #&lt;&lt; geom_point() + geom_smooth(method=&quot;lm&quot;) + ylab(&quot;tip %&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The result of this work would leave us with a model that could be used to impose a dining/tipping policy in restaurants (see here) and should also leave us with an unease that this policy is based on weak support. 2.2.2 What would be EDA 2.2.3 General strategy for EDA It’s a good idea to examine the data description, and the explanation of the variables. What does that look like here? - tip and totbill are quantitative, but generally we would think about tip as a percentage of the bill, so creating this new variable would be useful, too. - sex, smoker, day, time of day are categorical. - size is discrete/integer. The types of variables suggest what summaries to make. For the quantitative variables we would calculate - means, standard deviations, median, quartiles, minimum and maximum, and - also make histograms or density plots and scatterplots to explore bivariate relationships. For categorical variables, and integer variables with few categories, compute - counts and proportions, and - make bar charts and mosaic plots Thinking about relationships between categorical and quantitative variables, we could - calculate the numerical summaries by the categorical variable levels, and - make histograms and scatterplots, facetted by the categorical variables. Look at the distribution of tips, total bill. ggplot(tips, aes(x=tip)) + #&lt;&lt; geom_histogram( colour=&quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Because, one binwidth is never enough … ggplot(tips, aes(x=tip)) + geom_histogram( breaks=seq(0.5,10.5,1), #&lt;&lt; colour=&quot;white&quot;) + scale_x_continuous( breaks=seq(0,11,1)) Big fat bins. Tips are skewed, which means most tips are relatively small. ggplot(tips, aes(x=tip)) + geom_histogram( breaks=seq(0.5,10.5,0.1), #&lt;&lt; colour=&quot;white&quot;) + scale_x_continuous( breaks=seq(0,11,1)) Skinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts. Relationship between tip and total p &lt;- ggplot(tips, aes(x=totbill, y=tip)) + geom_point() + #&lt;&lt; scale_y_continuous( breaks=seq(0,11,1)) p Adding a regression line, helps to focus on a tipping standard, and whether individual tips are above or below expected. p &lt;- p + geom_abline(intercept=0, #&lt;&lt; slope=0.2) + #&lt;&lt; annotate(&quot;text&quot;, x=45, y=10, label=&quot;20% tip&quot;) p Examine the distributions across categorical variables. 2.2.4 Isn’t it data snooping? 2.2.5 Why aren’t there more resources on EDA? 2.3 The origins of EDA 2.4 Exercises Make a histogram of the total bill. Is there a similar pattern of multimodality? In the scatterplot of itp vs total bill, why is total on the horizontal axis? References "]]
