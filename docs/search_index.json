[["initial-data-analysis.html", "Chapter 3 Initial Data Analysis 3.1 Data description 3.2 Model formulation 3.3 Model formulation 3.4 Summary", " Chapter 3 Initial Data Analysis In Chapter 2, we mentioned that data analysis includes initial data analysis (IDA). There are various definitions of IDA, much like there are numerous definitions for EDA. Some people would be practising IDA without explicitly realising that it is IDA. Or in other cases, a different name is used to describe the same process, such as Chatfield (1985) referring to IDA also as “initial examination of data” and Cox &amp; Snell (1981) as “preliminary data anlysis” and Rao (1983) as “cross-examination of data.” So what is IDA? The two main objectives for initial data analysis are: data description, and model formulation. IDA differs from the main analysis (i.e. usually fitting the model, conducting significance tests, making inferences or predictions). IDA is often unreported in the data analysis reports or scientific papers due to it being “uninteresting” or “obvious.” The role of the main analysis is to answer the intended question(s) that the data were collected for. Sometimes IDA alone is sufficient. 3.1 Data description Data description should be one of the first steps in the data analysis to assess the structure and quality of the data. We refer them to occasionally as data sniffing or data scrutinizing. These include using common or domain knowledge to check if the recorded data have sensible values. E.g. Are positive values, e.g. height and weight, recorded as positive values with a plausible range? If the data are counts, are the recorded values contain noninteger values? For compositional data, do the values add up to 100% (or 1)? If not is that a measurement error or due to rounding? Or is another variable missing? In addition, numerical or graphical summaries may reveal that there is unwanted structure in the data. E.g., Does the treatment group have different demographic characteristics to the control group? Does the distribution of the data imply violations of assumptions for the main analysis? Data sniffing or data scrutinizing is a process that you get better at with practice and have familiarity with the domain area. Aside from checking the data structure or data quality, it’s important to check how the data are understood by the computer, i.e. checking for data type is also important. E.g., Was the date read in as character? Was a factor read in as numeric? 3.1.1 Checking the data type Example 1: Consider the following data stored as an excel sheet. We read this data into R as below. library(readxl) df &lt;- read_excel(&quot;data/example-data-type.xlsx&quot;) Below is a print out of the data object. Are there any issues here? df ## # A tibble: 5 x 4 ## id date loc temp ## &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2010-01-03 00:00:00 New York 42 ## 2 2 2010-02-03 00:00:00 New York 41.4 ## 3 3 2010-03-03 00:00:00 New York 38.5 ## 4 4 2010-04-03 00:00:00 New York 41.1 ## 5 5 2010-05-03 00:00:00 New York 39.8 In the United States, it is common to use the date format MM/DD/YYYY while the rest of the world commonly use DD/MM/YYYY or YYYY/MM/DD. It is highly probable that the dates are 1st-5th March and not 3rd of Jan-May. You can validate data with external sources, e.g. say the temperature at New York during the two choices suggest that the dates are 1st-5th March. The benefit of working with data grounded in the real world process is that there are generally means to sanity check. You can robustify your workflow by ensuring that you have an explicit check for the expected data type (and values) in your code. In the code below, we write our expected types and further coerce some data types to what we want. library(tidyverse) read_excel(&quot;data/example-data-type.xlsx&quot;, col_types = c(&quot;text&quot;, &quot;date&quot;, &quot;text&quot;, &quot;numeric&quot;)) %&gt;% mutate(id = as.factor(id), date = as.character(date), date = as.Date(date, format = &quot;%Y-%d-%m&quot;)) ## # A tibble: 5 x 4 ## id date loc temp ## &lt;fct&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2010-03-01 New York 42 ## 2 2 2010-03-02 New York 41.4 ## 3 3 2010-03-03 New York 38.5 ## 4 4 2010-03-04 New York 41.1 ## 5 5 2010-03-05 New York 39.8 read_csv(&quot;data/example-data-type.csv&quot;, col_types = cols(id = col_factor(), date = col_date(format = &quot;%m/%d/%y&quot;), loc = col_character(), temp = col_double())) ## # A tibble: 5 x 4 ## id date loc temp ## &lt;fct&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2010-03-01 New York 42 ## 2 2 2010-03-02 New York 41.4 ## 3 3 2010-03-03 New York 38.5 ## 4 4 2010-03-04 New York 41.1 ## 5 5 2010-03-05 New York 39.8 The checks (or coercions) ensure that even if the data are updated, you can have some confidence that any data type error will be picked up before further analysis. 3.1.2 Checking the data quality Numerical or graphical summaries, or even just eye-balling the data, helps to uncover some data quality issues. Do you see any issues for the data below? df2 &lt;- read_csv(&quot;data/example-data-quality.csv&quot;, col_types = cols(id = col_factor(), date = col_date(format = &quot;%m/%d/%y&quot;), loc = col_character(), temp = col_double())) df2 ## # A tibble: 9 x 4 ## id date loc temp ## &lt;fct&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 2010-03-01 New York 42 ## 2 2 2010-03-02 New York 41.4 ## 3 3 2010-03-03 New York 38.5 ## 4 4 2010-03-04 New York 41.1 ## 5 5 2010-03-05 New York 39.8 ## 6 6 2020-03-01 Melbourne 30.6 ## 7 7 2020-03-02 Melbourne 17.9 ## 8 8 2020-03-03 Melbourne 18.6 ## 9 9 2020-03-04 &lt;NA&gt; 21.3 There is a missing value in loc. Temperature is in Farenheit for New York but Celsius in Melbourne. You can find this out again using external sources. Consider the soybean study conducted in Brazil (cite Lehner 2016). This study collected multiple traits of soybeans grown at multiple locations over years. library(naniar) data(&quot;lehner.soybeanmold&quot;, package = &quot;agridat&quot;) vis_miss(lehner.soybeanmold) Inspecting the data reveals that there are a number of missing values for sclerotia and these values do not necessary appear missing at random. We could check if observational units that are missing values for sclerotia, have different yield say. ggplot(lehner.soybeanmold, aes(sclerotia, yield)) + geom_miss_point() We could also compare the new data with old data. library(inspectdf) soy_old &lt;- lehner.soybeanmold %&gt;% filter(year %in% 2010:2011) soy_new &lt;- lehner.soybeanmold %&gt;% filter(year == 2012) inspect_cor(soy_old, soy_new) %&gt;% show_plot() ## Warning: Columns with 0 variance found: year 3.1.3 Check on data collection method Next we study the data from ABS that shows the total number of people employed in a given month from February 1976 to December 2019 using the original time series. library(readabs) employed &lt;- read_abs(series_id = &quot;A84423085A&quot;) %&gt;% mutate(month = lubridate::month(date), year = factor(lubridate::year(date))) %&gt;% filter(year != &quot;2020&quot;) %&gt;% select(date, month, year, value) Do you notice anything? employed %&gt;% ggplot(aes(month, value, color = year)) + geom_line() + ggtitle(&quot;1978 Feb - 2019 Dec&quot;) Why do you think the number of people employed is going up each year? There’s a suspicious change in August numbers from 2014. A potential explanation for this is that there was a change in the survey from 2014. Also see https://robjhyndman.com/hyndsight/abs-seasonal-adjustment-2/ Check if the data collection method has been consistent. 3.1.4 Check for experimental data For experimental data, there would generally be some descriptions that include the experimental layout and any randomisation process of controlled variables (e.g. treatments) to units. Consider the experiment below. The experiment tests the effects of 9 fertilizer treatments on the yield of brussel sprouts on a field laid out in a rectangular array of 6 rows and 8 columns. df3 &lt;- read_csv(&quot;data/example-experimental-data.csv&quot;, col_types = cols(row = col_factor(), col = col_factor(), yield = col_double(), trt = col_factor(), block = col_factor())) df3 %&gt;% mutate(trt = fct_reorder(trt, yield)) %&gt;% ggplot(aes(trt, yield)) + geom_point(size = 4, alpha = 1 / 2) + guides(x = guide_axis(n.dodge = 2)) High sulphur and high manure seems to best for the yield of brussel sprouts. Any issues here? 3.2 Model formulation Note: there are variety of ways to do IDA and you don’t need to prescribe to what we show you. 3.2.1 Review: Linear models in R Consider the bivariate data cars were it contains the variables dist (distance travelled) and speed (speed of the car) with the data shown in Figure 3.1. Figure 3.1: The plot of the speed of cars vs. the distance travelled. thes models can be fitted in R as follows: lm(speed ~ dist, data = cars) which is the same as lm(speed ~ 1 + dist, data = cars) and for \\(i = 1, ... 50\\) mathematically can be written as \\[y_i = \\beta_0 + \\beta_1 x_i + e_i,\\] where: \\(y_i\\) and \\(x_i\\) are the speed (in mph) and stopping distance (in ft), respectively of the \\(i\\)-th car, \\(\\beta_0\\) and \\(\\beta_1\\) are intercept and slope, respectively, and \\(e_i\\) is the random error, typically assuming that \\(e_i \\sim N(0, \\sigma^2)\\). 3.3 Model formulation Say, we are interested in characterising the price of the diamond in terms of its carat. Looking at this plot, would you fit a linear model with formula price ~ 1 + carat? What about price ~ poly(carat, 2) which is the same as fitting: \\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + e_i.\\] Do we need to modify the assumption of the error distribution? Should we make some transformation before modelling? Are there other candidate models? By the way do you notice a wall like structure for prices at 1, 1.5 and 2 carat? Notice that we did no formal statistical inference as we initially try to formulate the model. The goal of the main analysis is to characterise the price of a diamond by its carat. This may involve: formal inference for model selection, justification of the selected “final” model, and fitting the final model. There may be in fact far more models considered but discarded at the IDA stage. These discarded models are hardly ever reported. Consequently, majority of reported statistics give a distorted view and it’s important to remind yourself what might not be reported. “All models are approximate and tentative; approximate in the sense that no model is exactly true and tentative in that they may be modified in the light of further data” – Chatfield (1985) “All models are wrong but some are useful” – George Box A wheat breeding trial to test 107 varieties (also called genotype) is conducted in a field experiment laid out in a rectangular array with 22 rows and 15 columns. data(&quot;gilmour.serpentine&quot;, package = &quot;agridat&quot;) 3.3.1 Experimental Design The experiment employs what is referred to as a randomised complete block design (RCBD) (technically it is near-complete and not exactly RCBD due to check varieties have double the replicates of test varieties). RCBD means that: the there are equal number of replicates for each treatment (here it is gen); each treatment appears exactly once in each block; the blocks are of the same size; and each treatment are randomised within block. In agricultural field experiments, blocks are formed spatially by grouping plots within contiguous areas (called rep here). The boundaries of blocks may be chosen arbitrary. In the main analysis, people would commonly analyse this using what is called two-way ANOVA model (with no interaction effect). The two-way ANOVA model has the form yield = mean + block + treatment + error So for this data, fit &lt;- lm(yield ~ 1 + rep + gen, data = gilmour.serpentine) summary(fit) ## ## Call: ## lm(formula = yield ~ 1 + rep + gen, data = gilmour.serpentine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -245.070 -69.695 -1.182 71.427 250.652 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 720.248 67.335 10.697 &lt; 2e-16 *** ## repR2 96.100 15.585 6.166 3.29e-09 *** ## repR3 -129.845 15.585 -8.331 8.44e-15 *** ## gen(WqKPWmH*3Ag 24.333 94.372 0.258 0.796766 ## genAMERY -93.333 94.372 -0.989 0.323747 ## genANGAS -132.667 94.372 -1.406 0.161192 ## genAROONA -153.667 94.372 -1.628 0.104884 ## genBATAVIA -175.333 94.372 -1.858 0.064513 . ## genBD231 -70.333 94.372 -0.745 0.456895 ## genBEULAH -173.667 94.372 -1.840 0.067074 . ## genBLADE -270.000 94.372 -2.861 0.004628 ** ## genBT_SCHOMBURG -49.000 94.372 -0.519 0.604125 ## genCADOUX -223.333 94.372 -2.367 0.018820 * ## genCONDOR -124.333 94.372 -1.317 0.189041 ## genCORRIGIN -217.667 94.372 -2.306 0.022010 * ## genCUNNINGHAM -254.667 94.372 -2.699 0.007502 ** ## genDGR/MNX-9-9e -47.667 94.372 -0.505 0.613996 ## genDOLLARBIRD -200.667 94.372 -2.126 0.034584 * ## genEXCALIBUR -55.000 94.372 -0.583 0.560621 ## genGOROKE -141.667 94.372 -1.501 0.134743 ## genHALBERD -53.333 94.372 -0.565 0.572551 ## genHOUTMAN -209.333 94.372 -2.218 0.027560 * ## genJANZ -214.667 94.372 -2.275 0.023884 * ## genK2011-5* -87.333 94.372 -0.925 0.355758 ## genKATUNGA -110.333 94.372 -1.169 0.243609 ## genKIATA -165.667 94.372 -1.755 0.080565 . ## genKITE -180.000 94.372 -1.907 0.057772 . ## genKULIN -91.000 94.372 -0.964 0.335964 ## genLARK -336.333 94.372 -3.564 0.000448 *** ## genLOWAN -152.333 94.372 -1.614 0.107915 ## genM4997 -146.000 94.372 -1.547 0.123277 ## genM5075 -194.667 94.372 -2.063 0.040304 * ## genM5097 -102.667 94.372 -1.088 0.277826 ## genMACHETE -231.333 94.372 -2.451 0.015010 * ## genMEERING -247.667 94.372 -2.624 0.009286 ** ## genMOLINEUX -165.667 94.372 -1.755 0.080565 . ## genOSPREY -162.000 94.372 -1.717 0.087451 . ## genOUYEN -136.667 94.372 -1.448 0.148986 ## genOXLEY -221.667 94.372 -2.349 0.019713 * ## genPELSART -200.333 94.372 -2.123 0.034882 * ## genPEROUSE -283.667 94.372 -3.006 0.002955 ** ## genRAC655 -112.667 94.372 -1.194 0.233813 ## genRAC655&#39;S&#39; -113.667 94.372 -1.204 0.229702 ## genRAC696 -3.667 94.372 -0.039 0.969042 ## genRAC710 -51.000 94.372 -0.540 0.589455 ## genRAC750 -77.333 94.372 -0.819 0.413410 ## genRAC759 -42.000 94.372 -0.445 0.656721 ## genRAC772 5.000 94.372 0.053 0.957794 ## genRAC777 -172.333 94.372 -1.826 0.069183 . ## genRAC779 3.667 94.372 0.039 0.969042 ## genRAC787 -118.000 94.372 -1.250 0.212486 ## genRAC791 -72.667 94.372 -0.770 0.442120 ## genRAC792 -102.333 94.372 -1.084 0.279385 ## genRAC798 -1.667 94.372 -0.018 0.985926 ## genRAC804 -45.000 94.372 -0.477 0.633949 ## genRAC805 -43.000 94.372 -0.456 0.649093 ## genRAC806 -35.333 94.372 -0.374 0.708462 ## genRAC807 -91.333 94.372 -0.968 0.334201 ## genRAC808 -54.000 94.372 -0.572 0.567765 ## genRAC809 -43.333 94.372 -0.459 0.646559 ## genRAC810 -131.667 94.372 -1.395 0.164359 ## genRAC811 42.333 94.372 0.449 0.654174 ## genRAC812 -94.000 94.372 -0.996 0.320310 ## genRAC813 -83.333 94.372 -0.883 0.378179 ## genRAC814 -72.333 94.372 -0.766 0.444214 ## genRAC815 -111.000 94.372 -1.176 0.240781 ## genRAC816 -66.333 94.372 -0.703 0.482862 ## genRAC817 -100.000 94.372 -1.060 0.290466 ## genRAC818 -107.000 94.372 -1.134 0.258101 ## genRAC819 -121.333 94.372 -1.286 0.199895 ## genRAC820 -1.000 94.372 -0.011 0.991555 ## genRAC821 -98.333 94.372 -1.042 0.298560 ## genROSELLA -184.333 94.372 -1.953 0.052050 . ## genSCHOMBURGK -132.333 94.372 -1.402 0.162242 ## genSHRIKE -128.000 94.372 -1.356 0.176376 ## genSPEAR -254.667 94.372 -2.699 0.007502 ** ## genSTILETTO -157.000 94.372 -1.664 0.097603 . ## genSUNBRI -218.333 94.372 -2.314 0.021612 * ## genSUNFIELD -206.667 94.372 -2.190 0.029576 * ## genSUNLAND -182.667 94.372 -1.936 0.054192 . ## genSWIFT -197.000 94.372 -2.087 0.037990 * ## genTASMAN -161.000 94.372 -1.706 0.089410 . ## genTATIARA -64.333 94.372 -0.682 0.496142 ## genTINCURRIN -19.000 81.728 -0.232 0.816382 ## genTRIDENT -132.667 94.372 -1.406 0.161192 ## genVF299 -66.333 94.372 -0.703 0.482862 ## genVF300 -111.667 94.372 -1.183 0.237976 ## genVF302 -108.333 94.372 -1.148 0.252234 ## genVF508 11.667 94.372 0.124 0.901725 ## genVF519 -1.000 94.372 -0.011 0.991555 ## genVF655 -160.167 81.728 -1.960 0.051283 . ## genVF664 -106.667 94.372 -1.130 0.259583 ## genVG127 -109.667 94.372 -1.162 0.246460 ## genVG503 -43.000 94.372 -0.456 0.649093 ## genVG506 -108.667 94.372 -1.151 0.250782 ## genVG701 -19.333 94.372 -0.205 0.837867 ## genVG714 -108.333 94.372 -1.148 0.252234 ## genVG878 52.333 94.372 0.555 0.579767 ## genWARBLER -217.000 94.372 -2.299 0.022415 * ## genWI216 4.000 94.372 0.042 0.966230 ## genWI221 -17.333 94.372 -0.184 0.854440 ## genWI231 -218.333 94.372 -2.314 0.021612 * ## genWI232 -56.333 94.372 -0.597 0.551165 ## genWILGOYNE -131.000 94.372 -1.388 0.166496 ## genWW1402 -117.333 94.372 -1.243 0.215071 ## genWW1477 -185.667 81.728 -2.272 0.024064 * ## genWW1831 -86.667 94.372 -0.918 0.359435 ## genWYUNA -176.667 94.372 -1.872 0.062524 . ## genYARRALINKA -245.000 94.372 -2.596 0.010061 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 115.6 on 221 degrees of freedom ## Multiple R-squared: 0.6226, Adjusted R-squared: 0.4381 ## F-statistic: 3.375 on 108 and 221 DF, p-value: 1.081e-14 Do you notice anything from below? It’s well known in agricultural field trials that spatial variations are introduced in traits; this could be because of the fertility trend, management practices or other reasons. In the IDA stage, you investigate to identify these spatial variations - you cannot just simply fit a two-way ANOVA model! Teaching of Statistics should provide a more balanced blend of IDA and inference – Chatfield (1985) Yet there is still very little emphasis of it in teaching and also at times in practice. So don’t forget to do IDA! 3.4 Summary Initial data analysis (IDA) is a model-focussed exploration of data with two main objectives: data description including scrutinizing for data quality, and model formulation without any formal statistical inference. IDA hardly sees the limelight even if it’s the very foundation of what the main analysis is built on. References "]]
